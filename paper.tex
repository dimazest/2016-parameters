\documentclass[11pt]{article}
\usepackage{acl2016}
\usepackage{times}

\usepackage{url}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{paralist}
\usepackage{mathtools}
\usepackage{dingbat}
\usepackage{subcaption}
\usepackage{balance}
\usepackage{gensymb}
\usepackage{marginnote}
\usepackage{adjustbox}

\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\makeatother
% \usepackage{hyperref}

\sloppy

% \aclfinalcopy % Uncomment this line for the final submission
\def\aclpaperid{34} %  Enter the acl Paper ID here


\usepackage{color}
\newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}

%\renewcommand{\baselinestretch}{0.95}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{Robust Co-occurrence Quantification for Lexical Distributional Semantics}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   {\tt email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   {\tt email@domain} \\}

\date{}

\newcommand{\BASEURL}{http://example.org}
% \newcommand{\BASEURL}{https://bitbucket.org/dimazest/phd-buildout/raw/tip/notebooks/downloads/compdistmeaning}
\newcommand{\dataurl}[1]{\href{\BASEURL/#1}{\nolinkurl{#1}}}

\newcommand{\p}{\textsuperscript{\textasteriskcentered}}
\newcommand{\pw}{\textsuperscript{\dag}}

\newcommand{\pp}{\textsuperscript\dag}
\newcommand{\ppp}{\textsuperscript\ddag}
\newcommand{\np}{\phantom{\textsuperscript\textasteriskcentered}}

\begin{document}

\def\PMI/{$\operatorname{1PMI}$}
\def\SPMI/{$\operatorname{1SPMI}$}
\def\CPMI/{$\operatorname{1CPMI}$}
\def\SCPMI/{$\operatorname{1SCPMI}$}

\def\NPMI/{$\operatorname{nPMI}$}
\def\NSPMI/{$\operatorname{nSPMI}$}
\def\NCPMI/{$\operatorname{nCPMI}$}
\def\NSCPMI/{$\operatorname{nSCPMI}$}

\def\logNPMI/{$\operatorname{lognPMI}$}
\def\logNSPMI/{$\operatorname{lognSPMI}$}
\def\logNCPMI/{$\operatorname{lognCPMI}$}
\def\logNSCPMI/{$\operatorname{lognSCPMI}$}

\maketitle
\begin{abstract}
Previous optimisations of parameters affecting the word-context association measure used in distributional vector space models have focused either on high-dimensional vectors with hundreds of thousands of dimensions, or dense vectors with dimensionality of few hundreds; but dimensionality of few thousands is often applied in compositional tasks as it is still computationally feasible and does not require the dimensionality reduction step. We present a systematic study of the interaction of the parameters of the association measure and vector dimensionality, and derive parameter selection heuristics that achieve performance across word similarity and relevance datasets competitive with the results previously reported in the literature achieved by highly dimensional or dense models.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\newcite{TACL570} propose optimisations for co-occurrence-based distributional models, using parameters adopted from predictive models \cite{mikolov2013efficient}: \emph{shifting} and \emph{context distribution smoothing}. Their experiments and thus their parameter  recommendations use high-dimensional vector spaces with word vector dimensionality of almost \textbf{200K}, and many recent state-of-the-art results in lexical distributional semantics have been obtained using vectors with similarly high dimensionality  \cite{baroni-dinu-kruszewski:2014:P14-1,kiela-clark:2014:CVSC,lapesa2014large}.

In contrast, much work on \emph{compositional} distributional semantics employs vectors with much fewer dimensions: e.g.~\textbf{2K} \cite{Grefenstette:2011:ESC:2145432.2145580,kartsadrqpl2014,milajevs-EtAl:2014:EMNLP2014}, \textbf{3K} \cite{Dinu:2010:MDS:1870658.1870771,milajevs-purver:2014:CVSC} or \textbf{10K} \cite{polajnar-clark:2014:EACL,Baroni2010nouns}. The most common reason thereof is that these models  assign tensors to functional words. For a vector space $V$ with $k$ dimensions,  a tensor $V \otimes V \cdots \otimes V$ of rank $n$ has $k^n$ dimensions. Adjectives and intransitive verbs have tensors of rank 2, transitive verbs are of rank 3; for coordinators, the rank can go up to 7. Taking $k = \textbf{200K}$ already results in a highly intractable tensor of $\textbf{8} \times \textbf{10}^{\textbf{15}}$ dimensions for a transitive verb. 

An alternative way of obtaining a vector space with few dimensions, usually with just 100--500, is to use SVD as a part of Latent Semantic Analysis \cite{ARIS:ARIS1440380105} or another models such as SGNS \cite{mikolov2013efficient} and GloVe \cite{pennington2014glove}. However, these models take more time to instantiate in comparison to weighting of a co-occurrence matrix and bring more parameters to explore. In this work we focus on vector spaces that directly weight a co-occurrence matrix and report results for SVD, GloVe and SGNS from the study of \newcite{TACL570} for comparison.

\input{figures/interaction-neg}

The mismatch of experiments with non-dense models in vector dimensionality between lexical and compositional tasks gives rise to a number of questions:
\begin{compactitem}
\item To what extent does model performance depend on vector dimensionality?
\item Do parameters influence 200K and 1K dimensional models similarly? Can the findings of \newcite{TACL570} be directly applied to models with few thousand dimensions?
\item If not, can we derive suitable parameter selection heuristics which take account of dimensionality?
\end{compactitem}

To answer these, we perform a systematic study of distributional models with a rich set of parameters on two lexical datasets: SimLex-999 \cite{hill2014simlex} and MEN \cite{Bruni:2014:MDS:2655713.2655714}.

\section{Parameters}
\label{sec:parameters}

\input{figures/interaction-cds}

\subsection{PMI variants (\texttt{discr})}
\label{sec:pmi-variants}

Most co-occurrence weighting schemes in distributional semantics are based on \emph{point-wise mutual information} (PMI, Equation~\ref{eq:pmi}) (see e.g.~\newcite{J90-1003}, \newcite{Turney:2010:FMV:1861751.1861756}, \newcite{NIPS2014_5477}).
%
\begin{equation}
  \label{eq:pmi}
  \operatorname{PMI}(x, y) = \log\frac{P(x,y)}{P(x)P(y)}
\end{equation}
%
%PMI in its raw form is problematic: non-observed co-occurrences lead to infinite PMI values, making it impossible to compute similarity. A common ``fix'' to this problem is to replace all infinities with zeroes, and we use PMI hereafter to refer to a weighting with this fix.
%
As commonly done, we replace the infinite PMI values with zeroes and use PMI hereafter to refer to a
weighting with this fix.
%
An alternative solution is to increment the probability ratio by 1; we refer to this as \textit{compressed PMI} (CPMI):
%
\begin{equation}
  \label{eq:cpmi}
  \operatorname{CPMI}(x, y) = \log\Big( 1 + \frac{P(x,y)}{P(x)P(y)} \Big)
\end{equation}

\input{figures/parameters}


\subsection{Shifted PMI (\texttt{neg})}
\label{sec:shifted-pmi}

Many approaches use only \emph{positive} PMI values, as  negative PMI values may not positively contribute to model performance \cite{Turney:2010:FMV:1861751.1861756}. This can be generalised to an additional cutoff parameter $k$ (\texttt{neg}) following \newcite{TACL570}, giving our third PMI variant (abbreviated as SPMI):
%
\begin{equation}
  \label{eq:ppmi}
  \operatorname{SPMI_k} = \max (0, \operatorname{PMI}(x, y) - \log k)
\end{equation}
%
We can apply the same idea to CPMI:
%
\begin{equation}
  \label{eq:pcpmi}
  \operatorname{SCPMI_k} = \max (0, \operatorname{CPMI}(x, y) - \log 2k)
\end{equation}

\subsection{Frequency weighting (\texttt{freq})}
\label{sec:frequency-weighting}

Another issue with PMI is its bias towards rare events \cite{TACL570}; one way of solving this issue is to weight the value by the co-occurrence frequency \cite{Evert05}:
%
\begin{equation}
  \label{eq:lmi}
  \operatorname{LMI}(x, y) = n(x, y)\operatorname{PMI}(x, y)
\end{equation}
%
where $n(x, y)$ is the number of times $x$ was seen together with $y$. For clarity, we refer to $n$-weighted PMIs as \NPMI/, \NSPMI/, etc. When this weighting component is set to 1, it has no effect; we can  explicitly label it as \PMI/, \SPMI/, etc.

In addition to the extreme $1$ and $n$ weightings, we also experiment with a $\log n$ weighting.

\subsection{Context distribution smoothing (\texttt{cds})}
\label{sec:cont-distr-smooth}

\newcite{TACL570} show that performance is affected by smoothing the context distribution $P(x)$:
%
\begin{equation}
  \label{eq:cds}
  P_{\alpha}(x) = \frac{n(x)^{\alpha}}{\sum_{c}n(c)^{\alpha}}
\end{equation}
%
We experiment with $\alpha$=1 (no smoothing) and $\alpha = 0.75$. We call this estimation method \emph{local context probability}; we can also estimate a \emph{global context probability} based on the size of the corpus $C$:
%
\begin{equation}
  \label{eq:cds-nan}
  P(x) = \frac{n(x)}{|C|}
\end{equation}

\subsection{Vector dimensionality ($D$)}
\label{sec:vect-dimens}

As context words we select the 1K, 2K, 3K, 5K, 10K, 20K, 30K, 40K and 50K most frequent lemmatised nouns, verbs, adjectives and adverbs. All context words are part of speech tagged, but we do not distinguish between refined word types (e.g. intransitive vs. transitive versions of verbs).

\section{Experiments}
\label{sec:lexical-experiments}

Table~\ref{tab:parameters} lists parameters and their values. As the source corpus we use the concatenation of Wackypedia and ukWaC \cite{Baroni2009} with a symmetric 5-word window \cite{milajevs-EtAl:2014:EMNLP2014}; our evaluation metric is the correlation with human judgements as is standard with SimLex \cite{hill2014simlex}. We derive our parameter selection heuristics by greedily selecting parameters (\texttt{cds}, \texttt{neg}) that lead to the highest average performance for each combination of frequency weighting, PMI variant and dimensionality $D$. Figures~\ref{fig:interaction-cds} and \ref{fig:interaction-neg} show the interaction of \texttt{cds} and \texttt{neg} with other parameters. We also vary the similarity measure (cosine and correlation  \cite{kiela-clark:2014:CVSC}), but do not report results here due to space limits.

\paragraph{PMI and CPMI}

PMI should be used with global context probabilities. CPMI generally outperforms PMI, with less sensitivity to parameters; \NCPMI/ and \logNCPMI/ should be used with local context probabilities and \CPMI/ should apply context distribution smoothing with $\alpha = 0.75$.

\paragraph{SPMI}

10K dimensional \SPMI/ is the least sensitive to parameter selection. For models with $D>$ 20K, context distribution smoothing should be used with $\alpha = 0.75$; for $D<$ 20K, it is beneficial to use global context probabilities. Shifting also depends on the dimensionality: models with $D<$ 20K should set $k = 0.7$, but higher-dimensional models should set $k = 5$. There might be a finer-grained $k$ selection criteria; however, we do not report this to avoid overfitting.

\input{figures/best}

\logNSPMI/ should be used with global context probabilities for models with $D<$ 20K. For higher-dimensional spaces, smoothing should be applied with $\alpha = 0.75$, as with \SPMI/. Shifting should be applied with $k = 0.5$ for models with $D<$ 20K,  and $k = 1.4$ for $D>$ 20K. In contrast to \SPMI/, which might require change of $k$ as the dimensionality increases, $k = 1.4$ is a much more robust choice for \logNSPMI/.

\NSPMI/ gives good results with local context probabilities ($\alpha = 1$). Models with $D<$ 20K  should use $k = 1.4$, otherwise $k = 5$ is preferred.

\paragraph{SCPMI}

With \SCPMI/ and $D<$ 20K, global context probability should be used, with shifting set to $k = 0.7$. Otherwise, local context probability should be used with $\alpha = 0.75$ and $k = 2$.

With \NSCPMI/ and $D<$ 20K, global context probability should be used with $k = 1.4$. Otherwise, local context probability without smoothing and $k = 5$ is suggested.

For \logNSCPMI/, models with $D<$ 20K should use global context probabilities and $k = 0.7$; otherwise, local context probabilities without smoothing should be preferred with $k = 1.4$.

\paragraph{Evaluation of heuristics}
\label{sec:heuristic-evaluation}

We evaluate these heuristics by comparing the performance they give on SimLex-999 against that obtained using the best possible parameter selections (determined via an exhaustive search at each dimensionality setting). We also compare them to the best scores reported by \newcite{TACL570} for their model, word2vec-SGNS \cite{mikolov2013efficient} and GloVe \cite{pennington2014glove} -- see Figure~\ref{fig:best-simlex} (only the better-performing SPMI and SCPMI are shown).

For \logNPMI/ and \logNCPMI/, our heuristics pick the best possible models. For \logNSPMI/, where performance variance is low, the heuristics do well, giving a performance of no more than 0.01 points below the best configuration. For \SPMI/ and \NSPMI/ the difference is higher. With \logNSCPMI/ and \SCPMI/, the heuristics follow the best selection, but with a wider gap than the SPMI models. In general $n$-weighted models do not perform as well as others.

Overall, $\log n$ weighting should be used with PMI, CPMI and SCPMI. High-dimensional SPMI models show the same behaviour, but if $D<$ 10K, no weighting should be applied. SPMI and SCPMI should be preferred over CPMI and PMI. As Figure~\ref{fig:global-best-simlex} shows, our heuristics give performance close to the optimum for any dimensionality, with a large improvement over both an average parameter setting and the parameters suggested by \cite{TACL570} in a high-dimensional setting.\footnote{Our results using \cite{TACL570}'s parameters differ slightly from theirs due to different window sizes (5 vs 2).}

Finally, to see whether the heuristics transfer robustly, we repeat this comparison on the MEN dataset (see Figures~\ref{fig:best-men}, \ref{fig:global-best-men}). Again, PMI and CPMI follow the best possible setup, with SPMI and SCPMI showing only a slight drop below ideal performance; and again, the heuristic settings give performance close to the optimum, and significantly higher than average or standard parameters.

\begin{table}
  \centering
  \begin{tabular}{lll}
    \toprule
    Model           & SimLex-999 & MEN \\
    \midrule
    PPMI$^*$        & 0.393      & 0.745 \\
    SVD$^*$         & 0.432      & 0.778 \\
    SGNS$^*$        & 0.438      & 0.774 \\
    GloVe$^*$       & 0.398      & 0.729 \\
    \addlinespace
    This work       & 0.385      & 0.765 \\
    \bottomrule
  \end{tabular}
  \caption{\textbf{Results.} Our model in comparison to the previous work. On the similarity dataset our model is 0.008 points behind a PPMI model, however on the relatedness dataset 0.020 points above, note the difference in dimensionality and source corpora, and window size. SVD and SGNS and GloVe numbers are given for comparison. $^*$Results reported by \newcite{TACL570}.}
\label{tab:results}
\end{table}


\section{Conclusion}
\label{sec:conclusion}

This paper presents a systematic study of co-occurrence quantification focusing on the selection of parameters presented in \newcite{TACL570}. We replicate their recommendation for high-dimensional vector spaces, and show that with appropriate parameter selection it is possible to achieve comparable performance with spaces of dimensionallity of 1K to 50K, and propose a set of model selection heuristics that maximizes performance. We foresee the results of the paper are generalisable to other experiments, since model selection was performed on a similarity dataset, and was additionally tested on a relatedness dataset.

In general, model performance depends on vector dimensionality (the best setup with 50K dimensions is better than the best setup with 1K dimensions by 0.03 on SimLex-999). Spaces with few thousand dimensions benefit from being dense and unsmoothed  ($k < 1$, global context probability); while high-dimensional spaces are better sparse and smooth  ($k > 1$, $\alpha = 0.75$). However, these heuristics do not guarantee the best possible result for unweighted and $n$-weighted models because of the high variance of the corresponding scores. Based on this we suggest the use of \logNSPMI/ or \logNSCPMI/ with dimensionality of at least 20K to ensure good performance on lexical tasks.

%\bibliographystyle{acl}
% remove publisher, month etc from conf proceedings:
% \bibliographystyle{acl-short}
\bibliographystyle{acl2016}
\balance
\bibliography{references,dmilajevs_publications}

\end{document}
